{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2903fd68",
   "metadata": {},
   "source": [
    "# Credit Risk Mini‑Case (Slides‑style Summary)\n",
    "**Dataset:** Give Me Some Credit (`cs-training.csv`)\n",
    "\n",
    "**Objective:** Build a PD model (baseline logistic regression), then design an approval cut‑off that maximizes **expected profit** under optional **risk appetite constraints**.\n",
    "\n",
    "**How to use in Colab:** upload `cs-training.csv`, then **Runtime → Run all**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "DATA_PATH = \"cs-training.csv\"  # change if needed\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.set_index(\"Unnamed: 0\")\n",
    "target = \"SeriousDlqin2yrs\"\n",
    "\n",
    "y = df[target].astype(int)\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "N = len(df)\n",
    "base_rate = y.mean()\n",
    "print(f\"Rows: {N:,} | Features: {X.shape[1]} | Base default rate: {base_rate:.2%}\")\n",
    "\n",
    "# Split (60/20/20, stratified)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=SEED, stratify=y)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp)\n",
    "\n",
    "print(\"Train/Valid/Test:\", X_train.shape, X_valid.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "def ks_stat(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return float(np.max(np.abs(tpr - fpr)))\n",
    "\n",
    "@dataclass\n",
    "class PreprocParams:\n",
    "    medians: pd.Series\n",
    "    q_low: pd.Series\n",
    "    q_high: pd.Series\n",
    "\n",
    "def fit_preproc_params(X_train: pd.DataFrame, low_q=0.01, high_q=0.99) -> PreprocParams:\n",
    "    med = X_train.median(numeric_only=True)\n",
    "    ql = X_train.quantile(low_q, numeric_only=True)\n",
    "    qh = X_train.quantile(high_q, numeric_only=True)\n",
    "    return PreprocParams(medians=med, q_low=ql, q_high=qh)\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, params: PreprocParams) -> pd.DataFrame:\n",
    "    Xp = X.copy()\n",
    "\n",
    "    # Missingness flags\n",
    "    for col in [\"MonthlyIncome\", \"NumberOfDependents\"]:\n",
    "        if col in Xp.columns:\n",
    "            Xp[f\"{col}_missing\"] = Xp[col].isna().astype(int)\n",
    "\n",
    "    # Delinquency flags\n",
    "    delin_cols = [\n",
    "        \"NumberOfTime30-59DaysPastDueNotWorse\",\n",
    "        \"NumberOfTime60-89DaysPastDueNotWorse\",\n",
    "        \"NumberOfTimes90DaysLate\",\n",
    "    ]\n",
    "    for col in delin_cols:\n",
    "        if col in Xp.columns:\n",
    "            Xp[f\"{col}_gt0\"] = (Xp[col].fillna(0) > 0).astype(int)\n",
    "\n",
    "    if all(col in Xp.columns for col in delin_cols):\n",
    "        Xp[\"any_delinquency\"] = (\n",
    "            (Xp[delin_cols[0]].fillna(0) > 0) |\n",
    "            (Xp[delin_cols[1]].fillna(0) > 0) |\n",
    "            (Xp[delin_cols[2]].fillna(0) > 0)\n",
    "        ).astype(int)\n",
    "\n",
    "    # Impute with train medians\n",
    "    for col in params.medians.index:\n",
    "        if col in Xp.columns:\n",
    "            Xp[col] = Xp[col].fillna(params.medians[col])\n",
    "\n",
    "    # Cap outliers with train quantiles\n",
    "    for col in params.q_low.index:\n",
    "        if col in Xp.columns:\n",
    "            Xp[col] = Xp[col].clip(params.q_low[col], params.q_high[col])\n",
    "\n",
    "    return Xp\n",
    "\n",
    "def decile_table(y_true: np.ndarray, score: np.ndarray, n_bins=10) -> pd.DataFrame:\n",
    "    d = pd.DataFrame({\"y\": np.asarray(y_true), \"score\": np.asarray(score)})\n",
    "    d[\"decile\"] = pd.qcut(d[\"score\"], q=n_bins, labels=False, duplicates=\"drop\") + 1  # 1 low risk, 10 high risk\n",
    "    out = (d.groupby(\"decile\", as_index=False)\n",
    "             .agg(n=(\"y\", \"size\"),\n",
    "                  bads=(\"y\", \"sum\"),\n",
    "                  avg_pd=(\"score\", \"mean\")))\n",
    "    out[\"bad_rate\"] = out[\"bads\"] / out[\"n\"]\n",
    "    out[\"share_of_bads\"] = out[\"bads\"] / out[\"bads\"].sum()\n",
    "    out[\"cum_share_of_bads\"] = out[\"share_of_bads\"].cumsum()\n",
    "    out = out.sort_values(\"decile\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def profit_curve(y_true, pd_score, R=15.0, L=300.0, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.001, 0.5, 400)  # practical PD range\n",
    "    y_true = np.asarray(y_true)\n",
    "    pd_score = np.asarray(pd_score)\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        approved = pd_score < t\n",
    "        if approved.sum() == 0:\n",
    "            continue\n",
    "        approval_rate = approved.mean()\n",
    "        bad_rate = y_true[approved].mean()\n",
    "        exp_profit_total = ((1 - pd_score[approved]) * R - pd_score[approved] * L).sum()\n",
    "        profit_per_applicant = exp_profit_total / len(pd_score)\n",
    "        rows.append([t, approval_rate, bad_rate, profit_per_applicant])\n",
    "    return pd.DataFrame(rows, columns=[\"threshold\", \"approval_rate\", \"bad_rate\", \"profit_per_applicant\"])\n",
    "\n",
    "def psi(expected: np.ndarray, actual: np.ndarray, n_bins: int = 10, eps: float = 1e-6) -> float:\n",
    "    expected = np.asarray(expected)\n",
    "    actual = np.asarray(actual)\n",
    "    expected = expected[~np.isnan(expected)]\n",
    "    actual = actual[~np.isnan(actual)]\n",
    "\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(expected, quantiles))\n",
    "    if len(bins) < 3:\n",
    "        return 0.0\n",
    "\n",
    "    e_counts, _ = np.histogram(expected, bins=bins)\n",
    "    a_counts, _ = np.histogram(actual,   bins=bins)\n",
    "\n",
    "    e_perc = e_counts / max(e_counts.sum(), 1)\n",
    "    a_perc = a_counts / max(a_counts.sum(), 1)\n",
    "\n",
    "    e_perc = np.clip(e_perc, eps, 1)\n",
    "    a_perc = np.clip(a_perc, eps, 1)\n",
    "\n",
    "    return float(np.sum((a_perc - e_perc) * np.log(a_perc / e_perc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55197159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (fit on TRAIN only)\n",
    "params = fit_preproc_params(X_train, low_q=0.01, high_q=0.99)\n",
    "Xtr = apply_preproc(X_train, params)\n",
    "Xva = apply_preproc(X_valid, params)\n",
    "Xte = apply_preproc(X_test,  params)\n",
    "\n",
    "# Model: Logistic Regression (industry baseline) + calibration\n",
    "base_model = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),  # safe even after preproc\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "cal_model = CalibratedClassifierCV(base_model, method=\"sigmoid\", cv=3)\n",
    "cal_model.fit(Xtr, y_train)\n",
    "\n",
    "p_test = cal_model.predict_proba(Xte)[:, 1]\n",
    "p_valid = cal_model.predict_proba(Xva)[:, 1]\n",
    "p_train = cal_model.predict_proba(Xtr)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "ks = ks_stat(y_test, p_test)\n",
    "\n",
    "psi_score = psi(p_train, p_test, n_bins=10)\n",
    "\n",
    "print(f\"Test ROC-AUC: {auc:.3f} | Test KS: {ks:.3f} | PSI(train→test score): {psi_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b31cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual 1: ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, p_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve (test)\")\n",
    "plt.show()\n",
    "\n",
    "# Visual 2: Calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, p_test, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure()\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"Mean predicted PD\")\n",
    "plt.ylabel(\"Observed default rate\")\n",
    "plt.title(\"Calibration (test, 10 quantile bins)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49beb650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual 3: Decile / lift table\n",
    "dec = decile_table(y_test, p_test, n_bins=10)\n",
    "display(dec)\n",
    "\n",
    "top = dec[dec[\"decile\"] == dec[\"decile\"].max()].iloc[0]  # highest risk decile\n",
    "print(f\"Highest-risk decile captures {top['share_of_bads']:.1%} of bads; bad rate = {top['bad_rate']:.1%}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(dec[\"decile\"], dec[\"bad_rate\"])\n",
    "plt.xlabel(\"Decile (1 lowest risk → 10 highest risk)\")\n",
    "plt.ylabel(\"Bad rate\")\n",
    "plt.title(\"Bad rate by PD decile (test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3615d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision policy: maximize expected profit (edit economics)\n",
    "R = 15.0    # profit per good approved loan\n",
    "L = 300.0   # loss per bad approved loan\n",
    "\n",
    "curve = profit_curve(y_test, p_test, R=R, L=L)\n",
    "\n",
    "best = curve.loc[curve[\"profit_per_applicant\"].idxmax()]\n",
    "print(\"Unconstrained optimum:\")\n",
    "display(best)\n",
    "\n",
    "# Optional constraints (risk appetite / growth)\n",
    "BAD_RATE_MAX = 0.02   # set None to disable\n",
    "APPROVAL_MIN = None   # e.g. 0.65, set None to disable\n",
    "\n",
    "best_c = best_under_constraint(curve, bad_rate_max=BAD_RATE_MAX, approval_min=APPROVAL_MIN)\n",
    "print(\"Best under constraints:\")\n",
    "display(best_c if best_c is not None else \"No feasible thresholds\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(curve[\"threshold\"], curve[\"profit_per_applicant\"], label=\"Profit per applicant\")\n",
    "plt.axvline(best[\"threshold\"], linestyle=\"--\", label=\"Unconstrained optimum\")\n",
    "if best_c is not None:\n",
    "    plt.axvline(best_c[\"threshold\"], linestyle=\"--\", label=\"Constrained optimum\")\n",
    "plt.xlabel(\"Threshold t (approve if PD < t)\")\n",
    "plt.ylabel(\"Expected profit per applicant\")\n",
    "plt.title(f\"Profit curve (test) | R={R}, L={L}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Champion vs Challenger (example champion threshold)\n",
    "def policy_metrics(y_true, pd_score, t, R, L):\n",
    "    y_true = np.asarray(y_true)\n",
    "    pd_score = np.asarray(pd_score)\n",
    "    approved = pd_score < t\n",
    "    approval_rate = approved.mean()\n",
    "    bad_rate = y_true[approved].mean() if approved.sum() > 0 else np.nan\n",
    "    exp_profit_total = ((1 - pd_score[approved]) * R - pd_score[approved] * L).sum() if approved.sum() > 0 else 0.0\n",
    "    return pd.Series({\n",
    "        \"threshold\": t,\n",
    "        \"approval_rate\": approval_rate,\n",
    "        \"bad_rate\": bad_rate,\n",
    "        \"profit_per_applicant\": exp_profit_total / len(pd_score),\n",
    "        \"profit_per_approved\": exp_profit_total / approved.sum() if approved.sum() > 0 else np.nan\n",
    "    })\n",
    "\n",
    "champion_t = 0.08\n",
    "champ = policy_metrics(y_test, p_test, champion_t, R, L)\n",
    "chall = policy_metrics(y_test, p_test, float(best[\"threshold\"]), R, L)\n",
    "\n",
    "comparison = pd.DataFrame([champ, chall], index=[\"Champion\", \"Challenger\"])\n",
    "display(comparison)\n",
    "\n",
    "uplift = (comparison.loc[\"Challenger\",\"profit_per_applicant\"] / comparison.loc[\"Champion\",\"profit_per_applicant\"]) - 1\n",
    "print(f\"Profit uplift per applicant (Challenger vs Champion): {uplift:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04112cf6",
   "metadata": {},
   "source": [
    "## Executive summary (ready for README / application)\n",
    "\n",
    "**Model (holdout test, random_state=42):**\n",
    "- Base rate: **6.68%**\n",
    "- ROC‑AUC: **0.860**\n",
    "- KS: **0.559**\n",
    "- PSI(score) train→test: **0.0004** *(proxy for drift monitoring)*\n",
    "\n",
    "**Risk segmentation (deciles by calibrated PD; Decile 1 = lowest risk, Decile 10 = highest risk):**\n",
    "- **Decile 10** captures **52.9%** of all bads with **35.4%** bad rate.\n",
    "- **Decile 1** bad rate is **0.40%**.\n",
    "\n",
    "**Decisioning & profitability (example economics):**\n",
    "- We optimize approval cut‑off using calibrated PDs and expected profit:  \n",
    "  `E[profit] = (1 − PD) * R − PD * L`\n",
    "- **Base scenario (R=15, L=300):**\n",
    "  - Champion (simple rule): **PD < 0.080** → approval **81.2%**, bad rate **2.52%**, profit/applicant **5.55**\n",
    "  - Challenger (profit‑optimal): **PD < 0.048** → approval **69.0%**, bad rate **1.72%**, profit/applicant **6.11**\n",
    "  - Profit uplift: **10.1%** vs champion *(same R/L units)*\n",
    "\n",
    "**Scenario sensitivity (profit‑optimal thresholds; and best under bad rate ≤2% when relevant):**\n",
    "| Scenario | R | L | Opt t | Opt appr | Opt bad | Opt $/app | Opt t (bad≤2%) | Appr (bad≤2%) | Bad (bad≤2%) | $/app (bad≤2%) |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| conservative | 15 | 600 | 0.024 | 51.2% | 1.02% | 3.12 | 0.024 | 51.2% | 1.02% | 3.12 |\n",
    "| base | 15 | 300 | 0.048 | 69.0% | 1.72% | 6.11 | 0.048 | 69.0% | 1.72% | 6.11 |\n",
    "| aggressive | 20 | 200 | 0.091 | 83.9% | 2.74% | 11.64 | 0.058 | 73.6% | 1.95% | 11.23 |\n",
    "\n",
    "**Monitoring checklist (production mindset):**\n",
    "- Weekly: approval rate, bad rate, expected profit KPIs\n",
    "- Model quality: AUC/KS stability (by cohort/vintage once timestamps exist)\n",
    "- Drift: PSI on score + key features; investigate when PSI is materially elevated\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
